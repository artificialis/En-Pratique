{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5fd3279e7128ab",
   "metadata": {},
   "source": [
    "# Speaker Embedding Visualization\n",
    "\n",
    "This notebook uses pyannote-audio to annotate audio files, extract speaker embeddings, and visualize them using dimensionality reduction techniques.\n",
    "\n",
    "The process involves:\n",
    "1. Loading audio files from the audio/ directory\n",
    "2. Using pyannote-audio for speaker diarization\n",
    "3. Extracting speaker embeddings for each segment\n",
    "4. Mapping speaker IDs to real names using speakers.json\n",
    "5. Visualizing the embeddings using T-SNE or UMAP\n",
    "\n",
    "In the visualization, points from the same speaker (real name) will have the same color, and points from the same audio file will have the same marker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edad86f4c8366a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2ebcb6d35b83e706",
   "metadata": {},
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# PyTorch   \n",
    "import torch\n",
    "\n",
    "# Pyannote.audio\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Rich for pretty printing\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "\n",
    "# Initialize console\n",
    "console = Console()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "748b5f4d1667b128",
   "metadata": {},
   "source": [
    "## 2. Load Speaker Mapping\n",
    "\n",
    "Load the mapping between speaker IDs and real names from speakers.json.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "68f2f407df42a6",
   "metadata": {},
   "source": [
    "# Load speaker mapping from speakers.json\n",
    "with open('speakers.json', 'r') as f:\n",
    "    speaker_mapping = json.load(f)\n",
    "\n",
    "# Display the mapping\n",
    "console.print(\"[bold]Speaker Mapping:[/bold]\")\n",
    "for audio_file, speakers in speaker_mapping.items():\n",
    "    console.print(f\"[cyan]{audio_file}[/cyan]\")\n",
    "    for speaker_id, speaker_name in speakers.items():\n",
    "        console.print(f\"  {speaker_id}: {speaker_name}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3914fe8d2edb7d5",
   "metadata": {},
   "source": [
    "## 3. Initialize Models\n",
    "\n",
    "Load the pre-trained speaker diarization and embedding models.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "104fac692dfa7120",
   "metadata": {},
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "console.print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained speaker diarization pipeline\n",
    "console.print(\"Loading pre-trained speaker diarization model...\")\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"HF_TOKEN_REDACTED\"\n",
    ")\n",
    "pipeline.to(device)\n",
    "\n",
    "# Load pre-trained speaker embedding model\n",
    "console.print(\"Loading pre-trained speaker embedding model...\")\n",
    "embedding_model = PretrainedSpeakerEmbedding(\n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Initialize audio processor\n",
    "audio = Audio(mono=\"downmix\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b414bb769f55cdd",
   "metadata": {},
   "source": [
    "## 4. Process Audio Files\n",
    "\n",
    "Process all WAV files in the audio/ directory, perform diarization, and extract speaker embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b340585cdc40cd0b",
   "metadata": {},
   "source": [
    "# Get all WAV files in the audio directory\n",
    "audio_files = sorted(glob.glob('audio/*.wav'))\n",
    "console.print(f\"Found {len(audio_files)} audio files: {audio_files}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4b742e0bd2f4bc0",
   "metadata": {},
   "source": [
    "# Initialize lists to store embeddings and metadata\n",
    "all_embeddings = []\n",
    "all_metadata = []\n",
    "\n",
    "# Process each audio file\n",
    "for audio_file in track(audio_files, description=\"Processing audio files\"):\n",
    "    # Get the base filename without path and extension\n",
    "    base_filename = os.path.basename(audio_file)\n",
    "    console.print(f\"\\n[bold]Processing {base_filename}[/bold]\")\n",
    "    \n",
    "    # Get file duration\n",
    "    file_duration = audio.get_duration(audio_file)\n",
    "    console.print(f\"File duration: {file_duration:.2f} seconds\")\n",
    "    \n",
    "    # Get speaker mapping for this file\n",
    "    if base_filename in speaker_mapping:\n",
    "        file_speaker_mapping = speaker_mapping[base_filename]\n",
    "    else:\n",
    "        console.print(f\"[red]Warning: No speaker mapping found for {base_filename}[/red]\")\n",
    "        file_speaker_mapping = {}\n",
    "    \n",
    "    # Perform diarization\n",
    "    console.print(f\"Performing diarization on {base_filename}...\")\n",
    "    diarization = pipeline(audio_file)\n",
    "    \n",
    "    # Process each segment\n",
    "    for turn, _, speaker_id in diarization.itertracks(yield_label=True):\n",
    "        # Get segment start and end times\n",
    "        seg_start = turn.start\n",
    "        seg_end = min(turn.end, file_duration)  # Ensure we don't go beyond file duration\n",
    "        \n",
    "        # Map speaker ID to real name\n",
    "        if speaker_id in file_speaker_mapping:\n",
    "            speaker_name = file_speaker_mapping[speaker_id]\n",
    "        else:\n",
    "            speaker_name = speaker_id  # Use ID if no mapping is found\n",
    "        \n",
    "        # Extract audio segment\n",
    "        speaker_segment = Segment(seg_start, seg_end)\n",
    "        waveform, sample_rate = audio.crop(audio_file, speaker_segment)\n",
    "        \n",
    "        # Extract embedding\n",
    "        embedding = embedding_model(waveform[None])\n",
    "        \n",
    "        # Store embedding and metadata\n",
    "        all_embeddings.append(embedding)\n",
    "        all_metadata.append({\n",
    "            'audio_file': base_filename,\n",
    "            'speaker_id': speaker_id,\n",
    "            'speaker_name': speaker_name,\n",
    "            'start_time': seg_start,\n",
    "            'end_time': seg_end,\n",
    "            'duration': seg_end - seg_start\n",
    "        })\n",
    "        \n",
    "        console.print(f\"  Segment: {seg_start:.2f}s - {seg_end:.2f}s, Speaker: {speaker_name} ({speaker_id})\")\n",
    "\n",
    "# Concatenate all embeddings\n",
    "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "console.print(f\"\\nTotal embeddings extracted: {all_embeddings.shape[0]}\")\n",
    "console.print(f\"Embedding dimension: {all_embeddings.shape[1]}\")\n",
    "\n",
    "# Create a DataFrame with metadata\n",
    "metadata_df = pd.DataFrame(all_metadata)\n",
    "\n",
    "# Add embeddings as a column in the DataFrame\n",
    "metadata_df['embedding'] = list(all_embeddings)\n",
    "\n",
    "# Verify the embedding column exists\n",
    "print(f\"DataFrame columns: {metadata_df.columns.tolist()}\")\n",
    "print(f\"First embedding shape: {metadata_df['embedding'].iloc[0].shape}\")\n",
    "\n",
    "metadata_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a844a49c8b7bc924",
   "metadata": {},
   "source": [
    "## 4.5 Compute Mean Embeddings per Speaker-Audio Combination\n",
    "\n",
    "Compute the mean embedding for each combination of speaker name and audio file. This step:\n",
    "1. Groups the embeddings by speaker name and audio file\n",
    "2. Calculates the mean embedding for each group\n",
    "3. Creates a new DataFrame with the mean embeddings and relevant metadata\n",
    "\n",
    "This allows us to compare speakers across different audio files by having a single representative embedding per speaker-audio combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd1bb3a921e29643",
   "metadata": {},
   "source": [
    "# Group by speaker name and audio file\n",
    "console.print(\"\\n[bold]Computing mean embeddings per speaker-audio combination...[/bold]\")\n",
    "\n",
    "# Initialize list to store mean embeddings and their metadata\n",
    "mean_embeddings = []\n",
    "mean_metadata = []\n",
    "\n",
    "# Group by speaker name and audio file\n",
    "for (speaker_name, audio_file), group in metadata_df.groupby(['speaker_name', 'audio_file']):\n",
    "    # Extract embeddings for this group\n",
    "    group_embeddings = np.array(group['embedding'].tolist())\n",
    "    \n",
    "    # Compute mean embedding\n",
    "    mean_emb = np.mean(group_embeddings, axis=0)\n",
    "    \n",
    "    # Store mean embedding and metadata\n",
    "    mean_embeddings.append(mean_emb)\n",
    "    mean_metadata.append({\n",
    "        'speaker_name': speaker_name,\n",
    "        'audio_file': audio_file,\n",
    "        'segment_count': len(group),\n",
    "        'total_duration': group['duration'].sum()\n",
    "    })\n",
    "    \n",
    "    console.print(f\"  {speaker_name} in {audio_file}: {len(group)} segments, {group['duration'].sum():.2f}s total\")\n",
    "\n",
    "# Create DataFrame with mean embeddings\n",
    "mean_df = pd.DataFrame(mean_metadata)\n",
    "\n",
    "# Add mean embeddings as a column\n",
    "mean_df['mean_embedding'] = list(mean_embeddings)\n",
    "\n",
    "# Display the mean embeddings DataFrame\n",
    "mean_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2c8449778e20351",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction\n",
    "\n",
    "Apply T-SNE to reduce the embeddings to 2 dimensions for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "84c348a0eb9fdab3",
   "metadata": {},
   "source": [
    "# Apply T-SNE for dimensionality reduction\n",
    "console.print(\"Applying T-SNE dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "embeddings_2d_tsne = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Add T-SNE coordinates to the DataFrame\n",
    "metadata_df['tsne_x'] = embeddings_2d_tsne[:, 0]\n",
    "metadata_df['tsne_y'] = embeddings_2d_tsne[:, 1]\n",
    "\n",
    "# Display the first few rows\n",
    "metadata_df.head()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bec28095b93b16dc",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Visualize the embeddings using T-SNE, with colors for speakers and markers for audio files.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8cb87c420b263f50",
   "metadata": {},
   "source": [
    "# Get unique speakers and audio files\n",
    "unique_speakers = metadata_df['speaker_name'].unique()\n",
    "unique_audio_files = metadata_df['audio_file'].unique()\n",
    "\n",
    "# Create color and marker mappings\n",
    "speaker_colors = {speaker: plt.cm.tab10(i % 10) for i, speaker in enumerate(unique_speakers)}\n",
    "file_markers = {file: marker for file, marker in zip(unique_audio_files, ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h'])}\n",
    "\n",
    "# Display the mappings\n",
    "console.print(\"\\n[bold]Speaker Color Mapping:[/bold]\")\n",
    "for speaker, color in speaker_colors.items():\n",
    "    console.print(f\"  {speaker}: {color}\")\n",
    "\n",
    "console.print(\"\\n[bold]Audio File Marker Mapping:[/bold]\")\n",
    "for file, marker in file_markers.items():\n",
    "    console.print(f\"  {file}: {marker}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77dcd81afbe491ee",
   "metadata": {},
   "source": [
    "# Function to create scatter plot\n",
    "def plot_embeddings(x, y, title, method):\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Plot each point\n",
    "    for speaker in unique_speakers:\n",
    "        for audio_file in unique_audio_files:\n",
    "            # Filter data for this speaker and audio file\n",
    "            mask = (metadata_df['speaker_name'] == speaker) & (metadata_df['audio_file'] == audio_file)\n",
    "            if mask.sum() > 0:\n",
    "                ax.scatter(\n",
    "                    x[mask], y[mask],\n",
    "                    color=speaker_colors[speaker],\n",
    "                    marker=file_markers[audio_file],\n",
    "                    label=f\"{speaker} - {audio_file}\",\n",
    "                    alpha=0.7,\n",
    "                    s=100\n",
    "                )\n",
    "    \n",
    "    # Create combined legend elements for audio files (markers) and speaker names (colors)\n",
    "    combined_legend_elements = []\n",
    "    \n",
    "    # Add audio file markers with their respective labels\n",
    "    for file, marker in file_markers.items():\n",
    "        combined_legend_elements.append(\n",
    "            Line2D([0], [0], marker=marker, color='gray', \n",
    "                  label=file, markersize=10, linestyle='None')\n",
    "        )\n",
    "    \n",
    "    # Add speaker colors with their respective labels\n",
    "    for speaker, color in speaker_colors.items():\n",
    "        combined_legend_elements.append(\n",
    "            Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                 label=speaker, markersize=10)\n",
    "        )\n",
    "    \n",
    "    # Create a second legend for additional information if needed\n",
    "    speaker_legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                                     label=speaker, markersize=10) \n",
    "                              for speaker, color in speaker_colors.items()]\n",
    "    \n",
    "    # Add the combined legend as the first legend\n",
    "    fig.legend(handles=combined_legend_elements, title=\"Audio Files (markers) and Speakers (colors)\", \n",
    "              loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    # Add a second legend below the first one if needed\n",
    "    # fig.legend(handles=speaker_legend_elements, title=\"Additional Info\", \n",
    "    #           loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(f\"{method} Dimension 1\", fontsize=12)\n",
    "    ax.set_ylabel(f\"{method} Dimension 2\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "623ae89add2974d",
   "metadata": {},
   "source": [
    "# Plot T-SNE visualization\n",
    "plot_embeddings(\n",
    "    metadata_df['tsne_x'], \n",
    "    metadata_df['tsne_y'], \n",
    "    \"Speaker Embeddings - T-SNE Projection\",\n",
    "    \"T-SNE\"\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3aedf6758ed1fb20",
   "metadata": {},
   "source": [
    "## 7. Analysis\n",
    "\n",
    "Let's analyze the results to see if speakers are well-separated in the embedding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6df21cfd988dc2f1",
   "metadata": {},
   "source": [
    "# Group by speaker and calculate statistics\n",
    "speaker_stats = metadata_df.groupby('speaker_name').agg({\n",
    "    'duration': ['count', 'sum', 'mean'],\n",
    "    'tsne_x': ['mean', 'std'],\n",
    "    'tsne_y': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Flatten the column names\n",
    "speaker_stats.columns = ['_'.join(col).strip() for col in speaker_stats.columns.values]\n",
    "speaker_stats = speaker_stats.reset_index()\n",
    "\n",
    "# Display statistics\n",
    "speaker_stats\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e6991cf-a843-49dd-a1a5-622f7a65b1e7",
   "metadata": {},
   "source": [
    "# Group by audio file and calculate statistics\n",
    "file_stats = metadata_df.groupby('audio_file').agg({\n",
    "    'duration': ['count', 'sum', 'mean'],\n",
    "    'speaker_name': 'nunique'\n",
    "})\n",
    "\n",
    "# Flatten the column names\n",
    "file_stats.columns = ['_'.join(col).strip() for col in file_stats.columns.values]\n",
    "file_stats = file_stats.reset_index()\n",
    "\n",
    "# Display statistics\n",
    "file_stats\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b444597c217f951",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f84e2a9f0b53c2b",
   "metadata": {},
   "source": [
    "## 11. Speaker Identification with ChromaDB\n",
    "\n",
    "In this section, we'll:\n",
    "1. Create a ChromaDB collection (deleting it if it already exists)\n",
    "2. Add the averaged speaker embeddings to the collection with metadata (audio file, speaker name)\n",
    "3. Diarize a test audio file and compute embeddings for each segment\n",
    "4. Query the ChromaDB collection to find the most similar speakers for each segment\n",
    "5. Display the results in a pandas dataframe\n",
    "\n",
    "This demonstrates how speaker embeddings can be used for speaker identification in a practical application."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb49c47c8e573e3d",
   "metadata": {},
   "source": [
    "\n",
    "# Import ChromaDB\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Define the path to the ChromaDB database\n",
    "DB_PATH = os.path.abspath(\"./chroma_db\")\n",
    "\n",
    "# Create a persistent ChromaDB client\n",
    "console.print(\"\\n[bold]Creating ChromaDB client...[/bold]\")\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Create or recreate the collection\n",
    "collection_name = \"speaker-embeddings\"\n",
    "console.print(f\"Creating collection: {collection_name}\")\n",
    "\n",
    "# Delete the collection if it already exists\n",
    "try:\n",
    "    client.delete_collection(collection_name)\n",
    "    console.print(f\"Deleted existing collection: {collection_name}\")\n",
    "except:\n",
    "    console.print(f\"No existing collection to delete: {collection_name}\")\n",
    "\n",
    "# Create a new collection\n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5cb7d7a090594d7",
   "metadata": {},
   "source": [
    "# Add the averaged speaker embeddings to the collection\n",
    "console.print(\"\\n[bold]Adding averaged speaker embeddings to ChromaDB...[/bold]\")\n",
    "\n",
    "# For each speaker-audio combination, add the mean embedding to the collection\n",
    "for idx, row in mean_df.iterrows():\n",
    "    speaker_name = row['speaker_name']\n",
    "    audio_file = row['audio_file']\n",
    "    embedding = row['mean_embedding']\n",
    "    \n",
    "    # Add to collection with metadata\n",
    "    collection.add(\n",
    "        ids=[f\"{speaker_name}_{audio_file}_{idx}\"],\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[{\n",
    "            \"speaker\": speaker_name,\n",
    "            \"audio_file\": audio_file,\n",
    "            \"segment_count\": int(row['segment_count']),\n",
    "            \"total_duration\": float(row['total_duration'])\n",
    "        }],\n",
    "        documents=[f\"Speaker: {speaker_name}, Audio: {audio_file}\"]\n",
    "    )\n",
    "    \n",
    "    console.print(f\"Added embedding for {speaker_name} from {audio_file}\")\n",
    "\n",
    "# Get collection count\n",
    "count = collection.count()\n",
    "console.print(f\"Total embeddings in collection: {count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93847146a0e33a7e",
   "metadata": {},
   "source": [
    "# Diarize the test audio file\n",
    "console.print(\"\\n[bold]Diarizing test audio file...[/bold]\")\n",
    "\n",
    "# Path to the test audio file\n",
    "test_audio_file = \"audio/test/locuteur_a_07.wav\"\n",
    "console.print(f\"Processing: {test_audio_file}\")\n",
    "\n",
    "# Get file duration\n",
    "file_duration = audio.get_duration(test_audio_file)\n",
    "console.print(f\"File duration: {file_duration:.2f} seconds\")\n",
    "\n",
    "# Perform diarization\n",
    "diarization = pipeline(test_audio_file)\n",
    "\n",
    "# Initialize lists to store segment data\n",
    "segment_data = []\n",
    "\n",
    "# Process each segment\n",
    "for turn, _, speaker_id in diarization.itertracks(yield_label=True):\n",
    "    # Get segment start and end times\n",
    "    seg_start = turn.start\n",
    "    seg_end = min(turn.end, file_duration)  # Ensure we don't go beyond file duration\n",
    "    \n",
    "    # Extract audio segment\n",
    "    speaker_segment = Segment(seg_start, seg_end)\n",
    "    waveform, sample_rate = audio.crop(test_audio_file, speaker_segment)\n",
    "    \n",
    "    # Extract embedding\n",
    "    embedding = embedding_model(waveform[None])\n",
    "    \n",
    "    # Query ChromaDB for similar speakers\n",
    "    results = collection.query(\n",
    "        query_embeddings=[embedding[0].tolist()],\n",
    "        n_results=3,\n",
    "        include=[\"distances\", \"metadatas\", \"documents\"]\n",
    "    )\n",
    "    \n",
    "    # Store segment data\n",
    "    segment_info = {\n",
    "        \"segment\": f\"Segment {len(segment_data) + 1}\",\n",
    "        \"start\": seg_start,\n",
    "        \"end\": seg_end,\n",
    "        \"speaker\": speaker_id,\n",
    "    }\n",
    "    \n",
    "    # Add top 3 similar speakers\n",
    "    for i in range(min(3, len(results[\"metadatas\"][0]))):\n",
    "        similarity = 1.0 - results[\"distances\"][0][i]\n",
    "        metadata = results[\"metadatas\"][0][i]\n",
    "        segment_info[f\"similar_{i+1}_speaker\"] = metadata[\"speaker\"]\n",
    "        segment_info[f\"similar_{i+1}_audio\"] = metadata[\"audio_file\"]\n",
    "        segment_info[f\"similar_{i+1}_similarity\"] = similarity\n",
    "    \n",
    "    segment_data.append(segment_info)\n",
    "    \n",
    "    console.print(f\"Processed segment: {seg_start:.2f}s - {seg_end:.2f}s, Speaker: {speaker_id}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9619faba7655bcb",
   "metadata": {},
   "source": [
    "# Create a DataFrame with the segment data\n",
    "console.print(\"\\n[bold]Creating DataFrame with segment data...[/bold]\")\n",
    "segments_df = pd.DataFrame(segment_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "console.print(\"Segment data:\")\n",
    "segments_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83701402de4c2f94",
   "metadata": {},
   "source": [
    "# Create a more readable visualization of the results\n",
    "console.print(\"\\n[bold]Speaker identification results:[/bold]\")\n",
    "\n",
    "# Format the DataFrame for better readability\n",
    "formatted_df = segments_df.copy()\n",
    "\n",
    "# Format time columns\n",
    "formatted_df[\"time_range\"] = formatted_df.apply(\n",
    "    lambda row: f\"{row['start']:.2f}s - {row['end']:.2f}s\", axis=1\n",
    ")\n",
    "\n",
    "# Format similarity columns\n",
    "for i in range(1, 4):\n",
    "    if f\"similar_{i}_similarity\" in formatted_df.columns:\n",
    "        formatted_df[f\"match_{i}\"] = formatted_df.apply(\n",
    "            lambda row: f\"{row[f'similar_{i}_speaker']} ({row[f'similar_{i}_audio']}) - {row[f'similar_{i}_similarity']:.4f}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "# Select and reorder columns\n",
    "display_columns = [\"segment\", \"time_range\", \"speaker\"]\n",
    "for i in range(1, 4):\n",
    "    if f\"match_{i}\" in formatted_df.columns:\n",
    "        display_columns.append(f\"match_{i}\")\n",
    "\n",
    "# Display the formatted DataFrame\n",
    "formatted_df[display_columns]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f6848f531884a9e",
   "metadata": {},
   "source": [
    "## 12. Success Rate Evaluation for Locuteur_A Identification\n",
    "\n",
    "Calculate the success rate for correctly identifying Locuteur_A in the test file using 1-NN, 2-NN, and 3-NN approaches.\n",
    "For each segment in the test file, we check if \"Locuteur_A\" is among the top k nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "48ead563ddde9d3d",
   "metadata": {},
   "source": [
    "# Calculate success rates for identifying Locuteur_A\n",
    "console.print(\"\\n[bold]Calculating success rates for Locuteur_A identification...[/bold]\")\n",
    "\n",
    "# Function to check if Locuteur_A is in the top k matches\n",
    "def is_locuteur_a_in_top_k(row, k):\n",
    "    for i in range(1, k+1):\n",
    "        if f\"similar_{i}_speaker\" in row and \"Locuteur_A\" in row[f\"similar_{i}_speaker\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Calculate success rates for k=1, k=2, and k=3\n",
    "total_segments = len(segments_df)\n",
    "success_counts = {1: 0, 2: 0, 3: 0}\n",
    "\n",
    "for k in [1, 2, 3]:\n",
    "    for _, row in segments_df.iterrows():\n",
    "        if is_locuteur_a_in_top_k(row, k):\n",
    "            success_counts[k] += 1\n",
    "    \n",
    "    success_rate = (success_counts[k] / total_segments) * 100\n",
    "    console.print(f\"{k}-NN Success Rate: {success_counts[k]}/{total_segments} segments ({success_rate:.2f}%)\")\n",
    "\n",
    "# Create a bar chart to visualize the success rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "k_values = list(success_counts.keys())\n",
    "success_rates = [(success_counts[k] / total_segments) * 100 for k in k_values]\n",
    "\n",
    "plt.bar(k_values, success_rates, color='skyblue')\n",
    "plt.xlabel('Number of Nearest Neighbors (k)', fontsize=12)\n",
    "plt.ylabel('Success Rate (%)', fontsize=12)\n",
    "plt.title('Success Rate for Identifying Locuteur_A with k-NN', fontsize=14)\n",
    "plt.xticks(k_values)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, rate in enumerate(success_rates):\n",
    "    plt.text(k_values[i], rate + 2, f'{rate:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1765321f72e9cb7c",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5cb73c2e4f6c8bf6",
   "metadata": {},
   "source": [
    "## 13. Processing Negative Class Audio File\n",
    "\n",
    "In this section, we'll:\n",
    "1. Diarize the negative class audio file (`audio/test/negative_class.wav`)\n",
    "2. Extract speaker embeddings for each segment\n",
    "3. Query the ChromaDB collection to find the 3 most similar speakers for each segment\n",
    "4. Display the results in a pandas dataframe\n",
    "5. Compute the Classification Rate for 1-NN, 2-NN, and 3-NN\n",
    "6. Show the average similarity of the 1-NN\n",
    "\n",
    "This demonstrates how speaker embeddings can be used for speaker identification with a negative class example.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a71b7e9ab7c3e06",
   "metadata": {},
   "source": [
    "# Diarize the negative class audio file\n",
    "console.print(\"\\n[bold]Diarizing negative class audio file...[/bold]\")\n",
    "\n",
    "# Path to the negative class audio file\n",
    "negative_audio_file = \"audio/test/negative_class.wav\"\n",
    "console.print(f\"Processing: {negative_audio_file}\")\n",
    "\n",
    "# Get file duration\n",
    "file_duration = audio.get_duration(negative_audio_file)\n",
    "console.print(f\"File duration: {file_duration:.2f} seconds\")\n",
    "\n",
    "# Perform diarization\n",
    "diarization = pipeline(negative_audio_file)\n",
    "\n",
    "# Initialize lists to store segment data\n",
    "negative_segment_data = []\n",
    "\n",
    "# Process each segment\n",
    "for turn, _, speaker_id in diarization.itertracks(yield_label=True):\n",
    "    # Get segment start and end times\n",
    "    seg_start = turn.start\n",
    "    seg_end = min(turn.end, file_duration)  # Ensure we don't go beyond file duration\n",
    "    \n",
    "    # Extract audio segment\n",
    "    speaker_segment = Segment(seg_start, seg_end)\n",
    "    waveform, sample_rate = audio.crop(negative_audio_file, speaker_segment)\n",
    "    \n",
    "    # Extract embedding\n",
    "    embedding = embedding_model(waveform[None])\n",
    "    \n",
    "    # Query ChromaDB for similar speakers\n",
    "    results = collection.query(\n",
    "        query_embeddings=[embedding[0].tolist()],\n",
    "        n_results=3,\n",
    "        include=[\"distances\", \"metadatas\", \"documents\"]\n",
    "    )\n",
    "    \n",
    "    # Store segment data\n",
    "    segment_info = {\n",
    "        \"segment\": f\"Segment {len(negative_segment_data) + 1}\",\n",
    "        \"start\": seg_start,\n",
    "        \"end\": seg_end,\n",
    "        \"speaker\": speaker_id,\n",
    "    }\n",
    "    \n",
    "    # Add top 3 similar speakers\n",
    "    for i in range(min(3, len(results[\"metadatas\"][0]))):\n",
    "        similarity = 1.0 - results[\"distances\"][0][i]\n",
    "        metadata = results[\"metadatas\"][0][i]\n",
    "        segment_info[f\"similar_{i+1}_speaker\"] = metadata[\"speaker\"]\n",
    "        segment_info[f\"similar_{i+1}_audio\"] = metadata[\"audio_file\"]\n",
    "        segment_info[f\"similar_{i+1}_similarity\"] = similarity\n",
    "    \n",
    "    negative_segment_data.append(segment_info)\n",
    "    \n",
    "    console.print(f\"Processed segment: {seg_start:.2f}s - {seg_end:.2f}s, Speaker: {speaker_id}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7eb5c6bf7b394b95",
   "metadata": {},
   "source": [
    "# Create a DataFrame with the segment data\n",
    "console.print(\"\\n[bold]Creating DataFrame with negative class segment data...[/bold]\")\n",
    "negative_segments_df = pd.DataFrame(negative_segment_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "console.print(\"Negative class segment data:\")\n",
    "negative_segments_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "330899bac0a57def",
   "metadata": {},
   "source": [
    "# Create a more readable visualization of the results\n",
    "console.print(\"\\n[bold]Speaker identification results for negative class:[/bold]\")\n",
    "\n",
    "# Format the DataFrame for better readability\n",
    "formatted_negative_df = negative_segments_df.copy()\n",
    "\n",
    "# Format time columns\n",
    "formatted_negative_df[\"time_range\"] = formatted_negative_df.apply(\n",
    "    lambda row: f\"{row['start']:.2f}s - {row['end']:.2f}s\", axis=1\n",
    ")\n",
    "\n",
    "# Format similarity columns\n",
    "for i in range(1, 4):\n",
    "    if f\"similar_{i}_similarity\" in formatted_negative_df.columns:\n",
    "        formatted_negative_df[f\"match_{i}\"] = formatted_negative_df.apply(\n",
    "            lambda row: f\"{row[f'similar_{i}_speaker']} ({row[f'similar_{i}_audio']}) - {row[f'similar_{i}_similarity']:.4f}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "# Select and reorder columns\n",
    "display_columns = [\"segment\", \"time_range\", \"speaker\"]\n",
    "for i in range(1, 4):\n",
    "    if f\"match_{i}\" in formatted_negative_df.columns:\n",
    "        display_columns.append(f\"match_{i}\")\n",
    "\n",
    "# Display the formatted DataFrame\n",
    "formatted_negative_df[display_columns]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b40528c8f6f15f13",
   "metadata": {},
   "source": [
    "## 14. Locuteur_A Rate Calculation for Negative Class\n",
    "\n",
    "Calculate the Locuteur_A Rate for 1-NN, 2-NN, and 3-NN for the negative class audio file.\n",
    "The Locuteur_A Rate is defined as the percentage of segments where Locuteur_A is identified among the top k nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab6378c42f554107",
   "metadata": {},
   "source": [
    "# Calculate Locuteur_A Rates for the negative class\n",
    "console.print(\"\\n[bold]Calculating Locuteur_A Rates for negative class...[/bold]\")\n",
    "\n",
    "# Calculate Locuteur_A Rates for k=1, k=2, and k=3\n",
    "total_negative_segments = len(negative_segments_df)\n",
    "locuteur_a_counts = {1: 0, 2: 0, 3: 0}\n",
    "\n",
    "for k in [1, 2, 3]:\n",
    "    for _, row in negative_segments_df.iterrows():\n",
    "        if is_locuteur_a_in_top_k(row, k):\n",
    "            locuteur_a_counts[k] += 1\n",
    "    \n",
    "    locuteur_a_rate = (locuteur_a_counts[k] / total_negative_segments) * 100\n",
    "    console.print(f\"{k}-NN Locuteur_A Rate: {locuteur_a_counts[k]}/{total_negative_segments} segments ({locuteur_a_rate:.2f}%)\")\n",
    "\n",
    "# Create a bar chart to visualize the Locuteur_A Rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "k_values = list(locuteur_a_counts.keys())\n",
    "locuteur_a_rates = [(locuteur_a_counts[k] / total_negative_segments) * 100 for k in k_values]\n",
    "\n",
    "plt.bar(k_values, locuteur_a_rates, color='salmon')\n",
    "plt.xlabel('Number of Nearest Neighbors (k)', fontsize=12)\n",
    "plt.ylabel('Locuteur_A Rate (%)', fontsize=12)\n",
    "plt.title('Locuteur_A Rate for Negative Class with k-NN', fontsize=14)\n",
    "plt.xticks(k_values)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, rate in enumerate(locuteur_a_rates):\n",
    "    plt.text(k_values[i], rate + 2, f'{rate:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4cc356a7c763a47e",
   "metadata": {},
   "source": [
    "## 15. Average Similarity of 1-NN for Negative Class\n",
    "\n",
    "Calculate and display the average similarity of the 1-NN (first nearest neighbor) for the negative class audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f173d4ab8cf778da",
   "metadata": {},
   "source": [
    "# Calculate average similarity of 1-NN for negative class\n",
    "console.print(\"\\n[bold]Calculating average similarity of 1-NN for negative class...[/bold]\")\n",
    "\n",
    "# Extract 1-NN similarities\n",
    "nn1_similarities = []\n",
    "for _, row in negative_segments_df.iterrows():\n",
    "    if \"similar_1_similarity\" in row:\n",
    "        nn1_similarities.append(row[\"similar_1_similarity\"])\n",
    "\n",
    "# Calculate average similarity\n",
    "if nn1_similarities:\n",
    "    avg_similarity = sum(nn1_similarities) / len(nn1_similarities)\n",
    "    console.print(f\"Average 1-NN similarity for negative class: {avg_similarity:.4f}\")\n",
    "    \n",
    "    # Create a histogram of 1-NN similarities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(nn1_similarities, bins=10, color='lightgreen', edgecolor='black')\n",
    "    plt.axvline(avg_similarity, color='red', linestyle='dashed', linewidth=2, label=f'Average: {avg_similarity:.4f}')\n",
    "    plt.xlabel('1-NN Similarity', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Distribution of 1-NN Similarities for Negative Class', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    console.print(\"No 1-NN similarities found for negative class.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "218edf382d2de958",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
